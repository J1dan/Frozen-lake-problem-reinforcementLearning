import numpy as np
from libraries.utility import first_visit_monte_carlo_control
from libraries.utility import sarsa
from libraries.utility import q_learning
from libraries.utility import get_policy

# Define the state-space and action-space
# states = [(i, j) for i in range(4) for j in range(4)]
actions = ['up', 'down', 'left', 'right']

# Define the reward function
reward_map = np.zeros((4, 4))
reward_map[1, 1] = -1
reward_map[1, 3] = -1
reward_map[2, 3] = -1
reward_map[3, 0] = -1
reward_map[3, 3] = 1

# Train the Monte Carlo first-visit control
Q_mc = first_visit_monte_carlo_control(reward_map, actions, 1000, 0.9)

# Train the SARSA with an ε-greedy behavior policy
Q_sarsa = sarsa(reward_map, actions, 1000, 0.9, 0.1, 0.1)

# Train the Q-learning with an ε-greedy behavior policy
Q_q = q_learning(reward_map, actions, 1000, 0.9, 0.1, 0.1)

policy_mc = get_policy(Q_mc)
policy_sarsa = get_policy(Q_sarsa)
policy_q = get_policy(Q_q)
print(policy_mc)
print(policy_sarsa)
print(policy_q)