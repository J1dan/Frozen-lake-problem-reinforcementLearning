import numpy as np
import matplotlib.pyplot as plt
from libraries.utility_extended import first_visit_monte_carlo_control
from libraries.utility_extended import sarsa
from libraries.utility_extended import q_learning
from libraries.utility_extended import get_policy
from libraries.utility_extended import evaluate_performance
from libraries.utility_extended import vis_policy

# Define the state-space and action-space
# states = [(i, j) for i in range(4) for j in range(4)]
# actions = ['up', 'down', 'left', 'right']
actions = ['up', 'down', 'left', 'right']
# Define the reward function
holes = [(0,5),(1, 1),(1, 4),(1, 8),(2, 4),(2, 5),(2, 8),(3, 3),(3, 5),(3, 9),(4, 0),(4, 1),(4, 4),(4, 7),(4, 8),(5, 1),(5, 7),(5, 9),(6, 3),(7, 4),(7, 9),(8, 4),(8, 6),(9, 2),(9, 6)]
reward_map = np.zeros((10, 10))
for r, c in holes:
    reward_map[r, c] = -1
reward_map[9, 9] = 1

# Train the Monte Carlo first-visit control
Q_mc, reward_per_episode_mc, accumulated_reward_mc, time_mc = first_visit_monte_carlo_control(holes, reward_map, actions, 500, 0.9, 0.5)
episodes_mc = range(len(reward_per_episode_mc))
actions_mc = range(len(accumulated_reward_mc))

# Train the SARSA with an ε-greedy behavior policy
Q_sarsa, reward_per_episode_sarsa, accumulated_reward_sarsa, time_sarsa = sarsa(holes, reward_map, actions, 500, 0.9, 0.1, 0.1)
episodes_sarsa = range(len(reward_per_episode_sarsa))
actions_sarsa = range(len(accumulated_reward_sarsa))

# Train the Q-learning with an ε-greedy behavior policy
Q_q, reward_per_episode_q, accumulated_reward_q, time_q = q_learning(holes, reward_map, actions, 500, 0.9, 0.1, 0.1)
episodes_q = range(len(accumulated_reward_sarsa))
actions_q = range(len(accumulated_reward_q))

actions = max(len(actions_sarsa),len(actions_q),len(actions_mc))

if actions == len(actions_sarsa):
    accumulated_reward_q.extend([0]*(actions - len(actions_q)))
    accumulated_reward_mc.extend([0]*(actions - len(actions_mc)))
elif actions == len(actions_mc):
    accumulated_reward_sarsa.extend([0]*(actions - len(actions_sarsa)))
    accumulated_reward_q.extend([0]*(actions - len(actions_q)))
else:
    accumulated_reward_mc.extend([0]*(actions - len(actions_mc)))
    accumulated_reward_sarsa.extend([0]*(actions - len(actions_sarsa)))
actions = range(actions)

policy_mc = get_policy(holes, Q_mc)
policy_sarsa = get_policy(holes, Q_sarsa)
policy_q = get_policy(holes, Q_q)
arrow_policy_mc = vis_policy(policy_mc)
arrow_policy_sarsa = vis_policy(policy_sarsa)
arrow_policy_q = vis_policy(policy_q)

print(arrow_policy_mc)
print(f"Processing time for First-visit Monte Carlo Control: {time_mc}")
print(arrow_policy_sarsa)
print(f"Processing time for SARSA: {time_sarsa}")
print(arrow_policy_q)
print(f"Processing time for Q-learning: {time_q}")

# create subplots
fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2)

ax1.plot(actions, accumulated_reward_mc, label = 'Accumulated reward of First-visit Monte Carlo')
ax1.plot(actions, accumulated_reward_sarsa, label = 'Accumulated reward of SARSA')
ax1.plot(actions, accumulated_reward_q, label = 'Accumulated reward of Q-learning')
ax1.legend()

ax2.plot(range(len(reward_per_episode_mc)), reward_per_episode_mc, label = 'Reward per episode of First-visit Monte Carlo')
ax2.plot(range(len(reward_per_episode_sarsa)), reward_per_episode_sarsa, label = 'Reward per episode of SARSA')
ax2.plot(range(len(reward_per_episode_q)), reward_per_episode_q, label = 'Reward per episode of Q-learning')
ax2.legend()
plt.show()